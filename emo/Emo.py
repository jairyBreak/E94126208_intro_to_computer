# -*- coding: utf-8 -*-
"""資訊報告.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ng-ly4GIAThukiYgaG8OyUmshVI2BYrj
"""

!pip3 install nltk
!pip install gradio
import nltk
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
from google.colab import files
#uploaded = files.upload()

!pip3 install nltk
!pip install gradio
import nltk
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
from textblob import TextBlob

# 定義要分析的文本
text = input("打一段字以表達心情")

# 使用TextBlob進行情緒分析
blob = TextBlob(text)
polarity = blob.sentiment.polarity
subjectivity = blob.sentiment.subjectivity


# 判斷情緒是正面還是負面
if polarity > 0:
    print("爽")
elif polarity < 0:
    print("破防")
else:
    print("還好")

# 輸出情緒極性和主觀性
print("Polarity:", polarity)
print("Subjectivity:", subjectivity)

from nltk.util import ngrams
from collections import Counter

with open("/content/drive/MyDrive/pythonTest/bbc_baboon.txt", 'r') as f:
    raw_news = f.read()
    #print(raw_news)
lower_news = raw_news.lower()
tokens = nltk.word_tokenize(lower_news)

#tokens = [nltk.tokenize.word_tokenize(sent) for sent in sentences]
#for token in tokens:
    #print(token)


news_unigrams = ngrams(tokens, 1)
news_unigrams_freq = Counter(news_unigrams)
print("Top 5 unigrams:\n{}".format(news_unigrams_freq.most_common(5)))

import nltk
from collections import Counter
from nltk.util import ngrams
import string
import collections
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')

stops = set(stopwords.words('english'))
stops |= set(string.punctuation)
stops.update(['``',"''","'s","also"])

raw_news = input()

text = raw_news.lower()
# Tokenize the text
tokens = nltk.word_tokenize(text)
filtered_words = [word for word in tokens if word.lower() not in stops]
polarity_list=[]

#print(filtered_words)
#print(tokens)
# Define the n-gram range
for word in filtered_words:
    analysis = TextBlob(word)
    polarity_list.append(analysis.sentiment.polarity)

#print(polarity_list)

positive = sum(1 for i in polarity_list if i > 0) / len(polarity_list) * 100
negative = sum(1 for i in polarity_list if i < 0) / len(polarity_list) * 100
neutral = sum(1 for i in polarity_list if i == 0) / len(polarity_list) * 100
fig = plt.figure()
x=[positive,negative,neutral]
plt.pie(x,radius=1.5,labels=["正面","負面","中性"],autopct='%.1f%%')
plt.show()
whole = TextBlob(text)
wholeSub = whole.sentiment.subjectivity
wholeEmo = whole.sentiment.polarity
print("正面的字數比例：",positive)
print("中性的字數比例：",neutral)
print("負面的字數比例：",negative)
print("整篇文章的情緒：",wholeEmo)
print("文章的主觀性：",wholeSub)
print(type(positive))
news_unigrams = ngrams(filtered_words, 1)
news_unigrams_freq = collections.Counter(news_unigrams)
print("出現最多的字詞:\n{}".format(news_unigrams_freq.most_common(5)))
#print(news_unigrams_freq)
# Print the n-grams
#for gram in grams:
    #print(gram)

!pip3 install nltk
!pip install gradio
#下載函式庫
import matplotlib.pyplot as plt
import nltk
import gradio as gr
from nltk.util import ngrams
import string
import collections
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')

stops = set(stopwords.words('english'))
stops |= set(string.punctuation)
stops.update(['``',"''","'s","also",'’','“','”'])#下載停用詞並加入標點符號（文章內要去除的文字與符號）

def emo(raw_news):
  text = raw_news
  tokens = nltk.word_tokenize(text)# 將每個詞分割
  filtered_words = [word for word in tokens if word.lower() not in stops]# 去除停用詞

  polarity_list=[]
  for word in filtered_words:
    analysis = TextBlob(word)#對每個字進行分析
    polarity_list.append(analysis.sentiment.polarity)#將每個字的情緒分數加入陣列裡

  #分析整篇文章的情緒
  whole = TextBlob(text)
  wholeSub = whole.sentiment.subjectivity
  wholeEmo = whole.sentiment.polarity

  #計算正負向字數
  positive = sum(1 for i in polarity_list if i > 0) / len(polarity_list) * 100
  negative = sum(1 for i in polarity_list if i < 0) / len(polarity_list) * 100
  neutral = sum(1 for i in polarity_list if i == 0) / len(polarity_list) * 100

  #fig = plt.figure()
  #x=[positive,negative,neutral]
  #plt.pie(x,radius=1.5,label=["正面","負面","中性"],autopct='%.1f%%')


  #計算出現最多次的字
  n=1
  news_unigrams = ngrams(filtered_words, n)
  news_unigrams_freq = collections.Counter(news_unigrams)

  #輸出結果並return
  outPosi = f"正面的字數比例：{positive}%"
  outNega = f"負面的字數比例：{negative}%"
  outNeut = f"中性的字數比例：{neutral}%"
  outSub = f"整篇文章的主觀程度：{wholeSub}"
  outEmo = f"整篇文章的情緒：{wholeEmo}"
  most=("出現最多的字詞:\n{}".kformat(news_unigrams_freq.most_common(5)))

  return outPosi,outNega,outNeut,outSub,outEmo,most

demo = gr.Interface(
    fn=emo,
    inputs=["text"],
    outputs=[ "text","text","text","text","text","text",],
    cache_examples=True
)#輸出可視化介面
demo.launch(share=True)

!pip install googletrans
import googletrans
from googletrans import Translator

translater = Translator()
out = translater.translate("你好", dest='en', src='auto')
print(out)